{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "* [0 -Importing Libraries](#0)\n",
    "* [1 - Read in df](#1)\n",
    "* [2 - PACEU](#2)\n",
    "* [3 - Sensitivy Analysis](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Imports <a class=\"anchor\" id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import ast  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Read in df <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = 'small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the df\n",
    "df = []\n",
    "\n",
    "# Define keys based on the train_train_train_train_train_dfset format\n",
    "keys = ['#*', '#@', '#t', '#c', '#index', '#%', '#!']\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open(txt_file, 'r', encoding='ISO-8859-1') as file:\n",
    "    current_paper = {}\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith('#*'):\n",
    "            # New paper starts; save the previous one if it exists\n",
    "            if current_paper:\n",
    "                df.append(current_paper)\n",
    "                current_paper = {}\n",
    "            current_paper['Title'] = line[2:].strip()\n",
    "        elif line.startswith('#@'):\n",
    "            current_paper['Authors'] = line[2:].strip()\n",
    "        elif line.startswith('#t'):\n",
    "            current_paper['Year'] = line[2:].strip()\n",
    "        elif line.startswith('#c'):\n",
    "            current_paper['Venue'] = line[2:].strip()\n",
    "        elif line.startswith('#index'):\n",
    "            current_paper['Index ID'] = line[6:].strip()\n",
    "        elif line.startswith('#%'):\n",
    "            if 'References' not in current_paper:\n",
    "                current_paper['References'] = []\n",
    "            current_paper['References'].append(line[2:].strip())\n",
    "        elif line.startswith('#!'):\n",
    "            current_paper['Abstract'] = line[2:].strip()\n",
    "        elif line.isdigit() and current_paper:  # Handle end of current paper\n",
    "            df.append(current_paper)\n",
    "            current_paper = {}\n",
    "    # Add the last paper\n",
    "    if current_paper:\n",
    "        df.append(current_paper)\n",
    "\n",
    "# Convert list of dicts to dfFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Replacing missing keys with None which becomes NaN in dfFrame\n",
    "for key in ['Title', 'Authors', 'Year', 'Venue', 'Index ID', 'References', 'Abstract']:\n",
    "    if key not in df.columns:\n",
    "        df[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take a random sample of 10% of the dataset\n",
    "# df_old = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_old.sample(frac=0.01).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "df = df[(df['Year']>1990) & (df['Year']<2010)]\n",
    "df['Index ID'] = pd.to_numeric(df['Index ID'], errors='coerce')\n",
    "# Apply the function to the dfFrame column\n",
    "df['References'] = df['References'].apply(\n",
    "    lambda x: [int(i) for i in x] if isinstance(x, list) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Cited Year (for SIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary from the dfFrame assuming 'Index ID' and 'Year' are columns in your dfFrame\n",
    "year_dict = df.set_index('Index ID')['Year'].to_dict()\n",
    "\n",
    "def get_last_cited_year(ref_ids):\n",
    "    if not isinstance(ref_ids, list):\n",
    "        return np.nan\n",
    "    # Filter and collect years where reference IDs exist in year_dict\n",
    "    years = [year_dict.get(int(ref_id)) for ref_id in ref_ids if int(ref_id) in year_dict]\n",
    "    return max(years) if years else np.nan\n",
    "\n",
    "# Apply the function to the 'References' column to compute the 'Last Cited Year'\n",
    "df['Last Cited Year'] = df['References'].apply(get_last_cited_year)\n",
    "df[\"Venue\"] = df[\"Venue\"].astype(str)\n",
    "df['Venue'] = df['Venue'].replace('', np.nan)\n",
    "df[\"References Count\"] = df[\"References\"].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total authors\n",
    "df['Authors'] = df['Authors'].fillna('')  # Fill empty strings where there are no authors\n",
    "unique_authors = set()\n",
    "df['Authors'].apply(lambda x: unique_authors.update(x.split(',')) if x else None)\n",
    "total_authors = len(unique_authors) - 1 if '' in unique_authors else len(unique_authors)\n",
    "\n",
    "# Fill missing Venue values with \"Unknown\"\n",
    "df['Venue'] = df['Venue'].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Index ID'] = pd.to_numeric(df['Index ID'], errors='coerce')\n",
    "\n",
    "# Convert 'References' from string representation of list to actual list\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    except:\n",
    "        return []  # Return an empty list if there's any error\n",
    "\n",
    "df['References'] = df['References'].apply(safe_eval)\n",
    "\n",
    "# Creating a dictionary of publication years for quick lookup\n",
    "publication_years = df.set_index('Index ID')['Year'].to_dict()\n",
    "\n",
    "# Function to count citations within three years\n",
    "def count_citations_within_3_years(row):\n",
    "    if isinstance(row['References'], list):  # Ensure that the data is list\n",
    "        citation_years = [publication_years.get(int(ref)) for ref in row['References'] if publication_years.get(int(ref)) is not None]\n",
    "        return sum((year is not None and (row['Year'] <= year <= row['Year'] + 3)) for year in citation_years)\n",
    "    return 0  # Return 0 if References is not a list\n",
    "\n",
    "# Apply the function\n",
    "df['citation_count_within_3_years'] = df.apply(count_citations_within_3_years, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.DataFrame([\n",
    "    {**row, 'Simulation_Year': year, 'State': 'Published' if year == row['Year'] else None}\n",
    "    for _, row in df.iterrows()\n",
    "    for year in range(row['Year'], 2010)\n",
    "])\n",
    "\n",
    "# Define the high-impact check as a function for clarity\n",
    "def is_high_impact(venue):\n",
    "    high_impact_venues = [\n",
    "        \"Nature\", \"Science\", \"IEEE Transactions on Information Forensics and Security\",\n",
    "        \"Journal of Machine Learning Research\", \"Communications of the ACM\",\n",
    "        \"IEEE Transactions on Knowledge and Data Engineering\", \"Journal of the ACM\",\n",
    "        \"Proceedings of the IEEE\", \"ACM Transactions on Algorithms\",\n",
    "        \"IEEE Trans. Parallel Distrib. Syst.\", \"IEEE Trans. Evolutionary Computation\",\n",
    "        \"Journal of Computational Physics\", \"Journal of the American Society for Information Science and Technology\"\n",
    "    ]\n",
    "    return venue in high_impact_venues\n",
    "\n",
    "# Determine initial states based on whether the paper is high impact and citations within 3 years\n",
    "def determine_initial_state(row):\n",
    "    if row['Simulation_Year'] == row['Year']:  # Only in the publication year\n",
    "        if is_high_impact(row['Venue']) and row['citation_count_within_3_years'] >= 5:\n",
    "            return 'Exposed'\n",
    "        return 'Published'\n",
    "    return None  # For all other years, initialize as None which will be updated later\n",
    "\n",
    "expanded_df['State'] = expanded_df.apply(determine_initial_state, axis=1)\n",
    "\n",
    "# You might want to fill forward the states based on the previous state\n",
    "# to ensure continuity until explicitly changed by state update logic\n",
    "expanded_df.sort_values(by=['Index ID', 'Simulation_Year'], inplace=True)\n",
    "expanded_df['State'] = expanded_df.groupby('Index ID')['State'].ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488903"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize states only for the publication year\n",
    "def determine_initial_state(row):\n",
    "    if row['High_Impact'] and row['citation_count_within_3_years'] >= 5:\n",
    "        return 'Exposed'\n",
    "    return 'Published'\n",
    "\n",
    "# Apply initial state only in the publication year\n",
    "expanded_df['State'] = expanded_df.apply(lambda row: determine_initial_state(row) if row['Simulation_Year'] == row['Year'] else np.nan, axis=1)\n",
    "\n",
    "# Ensure DataFrame is sorted by 'Index ID' and 'Simulation_Year' for correct state propagation\n",
    "expanded_df.sort_values(by=['Index ID', 'Simulation_Year'], inplace=True)\n",
    "\n",
    "# Forward fill states to carry previous valid state forward until a new state is defined\n",
    "expanded_df['State'] = expanded_df.groupby('Index ID')['State'].ffill()\n",
    "\n",
    "# Define the vectorized update function to apply state transitions\n",
    "def apply_state_transitions(df):\n",
    "    # Update states based on previous state\n",
    "    conditions = [\n",
    "        (df['State'] == 'Published') & (df['citation_count_within_3_years'] >= 5),\n",
    "        (df['State'] == 'Published') & (df['References'].apply(lambda x: isinstance(x, list))),\n",
    "        (df['State'] == 'Exposed') & ((df['Simulation_Year'] - df['Year']) > 3) & (df['citation_count_within_3_years'] < 5)\n",
    "    ]\n",
    "    choices = ['Exposed', 'Cited', 'Archived']\n",
    "    df['State'] = np.select(conditions, choices, default=df['State'])\n",
    "    return df\n",
    "\n",
    "# Apply state transitions across the dataframe\n",
    "expanded_df = apply_state_transitions(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 208.60 ms\n"
     ]
    }
   ],
   "source": [
    "# Ensure the dataframe is correctly sorted\n",
    "expanded_df.sort_values(by=['Index ID', 'Simulation_Year'], inplace=True)\n",
    "\n",
    "# Precompute whether references are a list for use in conditions\n",
    "expanded_df['References_Is_List'] = expanded_df['References'].apply(lambda x: isinstance(x, list))\n",
    "\n",
    "# Create a new column for the previous state only once\n",
    "expanded_df['Previous_State'] = expanded_df.groupby('Index ID')['State'].shift(1)\n",
    "\n",
    "# Define conditions using vectorized operations for better performance\n",
    "def vectorized_update_state(df):\n",
    "    conditions = [\n",
    "        (df['Previous_State'] == 'Published') & (df['citation_count_within_3_years'] >= 5) & df['References_Is_List'],\n",
    "        (df['Previous_State'] == 'Exposed') & ((df['Simulation_Year'] - df['Year']) > 3) & (df['citation_count_within_3_years'] < 5)\n",
    "    ]\n",
    "    choices = ['Exposed', 'Archived']\n",
    "    df['State'] = np.select(conditions, choices, default=df['Previous_State'])\n",
    "    return df\n",
    "\n",
    "# Filter for active papers only once\n",
    "active_papers_df = expanded_df[expanded_df['State'].isin(['Published', 'Exposed', 'Cited'])]\n",
    "\n",
    "# Convert the active DataFrame to a Dask DataFrame for parallel processing\n",
    "dask_df = dd.from_pandas(active_papers_df, npartitions=10)\n",
    "\n",
    "# Set up the progress bar to monitor the compute operation\n",
    "with ProgressBar():\n",
    "    # Apply the vectorized update function in parallel and compute the results\n",
    "    final_active_df = dask_df.map_partitions(vectorized_update_state).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Temp\\ipykernel_33568\\2867724789.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat([final_active_df, expanded_df[~expanded_df.index.isin(final_active_df.index)]])\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.concat([final_active_df, expanded_df[~expanded_df.index.isin(final_active_df.index)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) PACEU model <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df = final_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = 2000\n",
    "test_year = 2007\n",
    "max_year = 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split\n",
    "train_df = df[(df['Year'] <= test_year) & (df['Year'] > min_year)]\n",
    "test_df = df[df['Year'] > test_year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Define model and calculate transition rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citation_model(y, t, lambda_PE, lambda_EC, lambda_CH, gamma_HA):\n",
    "    P, E, C, H, A = y\n",
    "    dP_dt = -lambda_PE * P  # Rate at which papers move from PENDING to EXPOSED\n",
    "    dE_dt = lambda_PE * P - lambda_EC * E  # New papers becoming EXPOSED and some moving to CITED\n",
    "    dC_dt = lambda_EC * E - lambda_CH * C  # EXPOSED papers moving to CITED or to HIGH IMPACT\n",
    "    dH_dt = lambda_CH * C - gamma_HA * H  # CITED papers becoming HIGH IMPACT and some moving to ARCHIVED\n",
    "    dA_dt = gamma_HA * H  # HIGH IMPACT papers becoming ARCHIVED\n",
    "    return [dP_dt, dE_dt, dC_dt, dH_dt, dA_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Temp\\ipykernel_33568\\3263877288.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Next_Year'] = data['Year'] + 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Exposed'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rate\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate parameter estimates for each transition\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m lambda_PE \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_transition_rates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPublished\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExposed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m lambda_EC \u001b[38;5;241m=\u001b[39m calculate_transition_rates(train_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExposed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCited\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m lambda_CH \u001b[38;5;241m=\u001b[39m calculate_transition_rates(train_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCited\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh Impact\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m, in \u001b[0;36mcalculate_transition_rates\u001b[1;34m(data, from_state, to_state)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_transition_rates\u001b[39m(data, from_state, to_state):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Adjust data to match state names and logic\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNext_Year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     from_next_year \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndex ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNext_Year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNext_Year\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, to_state: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo_State_Next_Year\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      5\u001b[0m     merged \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmerge(from_next_year, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Calculate transitions based on the new state definitions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Exposed'] not in index\""
     ]
    }
   ],
   "source": [
    "def calculate_transition_rates(data, from_state, to_state):\n",
    "    # Adjust data to match state names and logic\n",
    "    data['Next_Year'] = data['Year'] + 1\n",
    "    from_next_year = data[['Index ID', 'Next_Year', to_state]].rename(columns={'Next_Year': 'Year', to_state: 'To_State_Next_Year'})\n",
    "    merged = data.merge(from_next_year, on=['Index ID', 'Year'], how='left')\n",
    "    \n",
    "    # Calculate transitions based on the new state definitions\n",
    "    transitions = merged[(merged['State'] == from_state) & (merged['To_State_Next_Year'] == to_state)]\n",
    "    total_from_state = merged[merged['State'] == from_state]\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    rate = len(transitions) / len(total_from_state) if len(total_from_state) > 0 else 0\n",
    "    return rate\n",
    "\n",
    "# Calculate parameter estimates for each transition\n",
    "lambda_PE = calculate_transition_rates(train_df, 'Published', 'Exposed')\n",
    "lambda_EC = calculate_transition_rates(train_df, 'Exposed', 'Cited')\n",
    "lambda_CH = calculate_transition_rates(train_df, 'Cited', 'High Impact')\n",
    "gamma_HA = calculate_transition_rates(train_df, 'High Impact', 'Archived')\n",
    "\n",
    "# Print the estimated parameters\n",
    "print(f\"Estimated lambda_PE: {lambda_PE}\")\n",
    "print(f\"Estimated lambda_EC: {lambda_EC}\")\n",
    "print(f\"Estimated lambda_CH: {lambda_CH}\")\n",
    "print(f\"Estimated gamma_HA: {gamma_HA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For R0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_probability(data, from_state, to_state):\n",
    "    # Filter data for relevant transitions\n",
    "    relevant_data = data[data['State'] == from_state]\n",
    "    transitions = relevant_data[relevant_data['Next_State'] == to_state]\n",
    "\n",
    "    # Calculate transition probability\n",
    "    probability = len(transitions) / len(relevant_data) if len(relevant_data) > 0 else 0\n",
    "    return probability\n",
    "\n",
    "# Calculate the probability of moving from Exposed to High Impact directly\n",
    "probability_EH = calculate_transition_probability(expanded_df, 'Exposed', 'High Impact')\n",
    "print(f\"Probability of moving from Exposed to High Impact: {probability_EH:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Set initial conditions and run on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions based on the first year data in training set\n",
    "initial_conditions_train = [\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('Published', 0),\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('Exposed', 0),\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('Cited', 0),\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('High Impact', 0)\n",
    "]\n",
    "\n",
    "# Time points\n",
    "time_points_train = np.arange(min_year, test_year + 1)\n",
    "\n",
    "# Solve ODE for training data\n",
    "results_train = odeint(citation_model, initial_conditions_train, time_points_train, args=(lambda_PE, lambda_EC, lambda_CH, gamma_HA))\n",
    "\n",
    "# Gather actual data for training period\n",
    "actual_data_train = [\n",
    "    [train_df[train_df['Year'] == year]['State'].value_counts().get('Published', 0),\n",
    "     train_df[train_df['Year'] == year]['State'].value_counts().get('Exposed', 0),\n",
    "     train_df[train_df['Year'] == year]['State'].value_counts().get('Cited', 0),\n",
    "     train_df[train_df['Year'] == year]['State'].value_counts().get('High Impact', 0)] for year in time_points_train\n",
    "]\n",
    "\n",
    "# Calculate MSE for training\n",
    "mse_train = np.mean((np.array(actual_data_train) - results_train)**2)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(time_points_train, results_train, marker='o', label=['Simulated Published', 'Simulated Exposed', 'Simulated Cited', 'Simulated High Impact'])\n",
    "plt.plot(time_points_train, np.array(actual_data_train), linestyle='--', label=['Actual Published', 'Actual Exposed', 'Actual Cited', 'Actual High Impact'])\n",
    "plt.title('Training Set: Actual vs. Simulated Data')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Squared Error for Training Data: {mse_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions based on last year of training data\n",
    "initial_conditions_test = results_train[-1, :]\n",
    "\n",
    "# Time points for testing data\n",
    "time_points_test = np.arange(test_year + 1, max_year + 1)\n",
    "\n",
    "# Solve ODE for testing data\n",
    "results_test = odeint(citation_model, initial_conditions_test, time_points_test, args=(lambda_PE, lambda_EC, lambda_CH, gamma_HA))\n",
    "\n",
    "# Gather actual data for testing period\n",
    "actual_data_test = [\n",
    "    [test_df[test_df['Year'] == year]['State'].value_counts().get('Published', 0),\n",
    "     test_df[test_df['Year'] == year]['State'].value_counts().get('Exposed', 0),\n",
    "     test_df[test_df['Year'] == year]['State'].value_counts().get('Cited', 0),\n",
    "     test_df[test_df['Year'] == year]['State'].value_counts().get('High Impact', 0)] for year in time_points_test\n",
    "]\n",
    "\n",
    "# Calculate MSE for testing\n",
    "mse_test = np.mean((np.array(actual_data_test) - results_test)**2)\n",
    "\n",
    "# Plot testing results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(time_points_test, results_test, marker='o', label=['Simulated Published', 'Simulated Exposed', 'Simulated Cited', 'Simulated High Impact'])\n",
    "plt.plot(time_points_test, np.array(actual_data_test), linestyle='--', label=['Actual Published', 'Actual Exposed', 'Actual Cited', 'Actual High Impact'])\n",
    "plt.title('Testing Set: Actual vs. Simulated Data')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Squared Error for Testing Data: {mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sensitivity <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range for each parameter\n",
    "lambda_PE_values = np.linspace(0.05, 0.15, 5)\n",
    "lambda_EC_values = np.linspace(0.02, 0.08, 5)\n",
    "lambda_CH_values = np.linspace(0.01, 0.05, 5)\n",
    "gamma_HA_values = np.linspace(0.005, 0.015, 5)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for lambda_PE in lambda_PE_values:\n",
    "    for lambda_EC in lambda_EC_values:\n",
    "        for lambda_CH in lambda_CH_values:\n",
    "            for gamma_HA in gamma_HA_values:\n",
    "                # Solve ODE with current parameters\n",
    "                results_sim = odeint(citation_model, initial_conditions_train, time_points_train, args=(lambda_PE, lambda_EC, lambda_CH, gamma_HA))\n",
    "                mse_sim = np.mean((np.array(actual_data_train) - results_sim)**2)\n",
    "                results.append((lambda_PE, lambda_EC, lambda_CH, gamma_HA, mse_sim))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['lambda_PE', 'lambda_EC', 'lambda_CH', 'gamma_HA', 'MSE'])\n",
    "\n",
    "# Find the parameters that minimize MSE\n",
    "min_mse_params = results_df.loc[results_df['MSE'].idxmin()]\n",
    "\n",
    "print(\"Optimal parameters:\")\n",
    "print(min_mse_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
