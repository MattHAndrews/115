{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "* [0 -Importing Libraries](#0)\n",
    "* [1 - Read in df](#1)\n",
    "* [2 - PACEU](#2)\n",
    "* [3 - Sensitivy Analysis](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Imports <a class=\"anchor\" id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import ast  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Read in df <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = 'small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the df\n",
    "df = []\n",
    "\n",
    "# Define keys based on the train_train_train_train_train_dfset format\n",
    "keys = ['#*', '#@', '#t', '#c', '#index', '#%', '#!']\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open(txt_file, 'r', encoding='ISO-8859-1') as file:\n",
    "    current_paper = {}\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith('#*'):\n",
    "            # New paper starts; save the previous one if it exists\n",
    "            if current_paper:\n",
    "                df.append(current_paper)\n",
    "                current_paper = {}\n",
    "            current_paper['Title'] = line[2:].strip()\n",
    "        elif line.startswith('#@'):\n",
    "            current_paper['Authors'] = line[2:].strip()\n",
    "        elif line.startswith('#t'):\n",
    "            current_paper['Year'] = line[2:].strip()\n",
    "        elif line.startswith('#c'):\n",
    "            current_paper['Venue'] = line[2:].strip()\n",
    "        elif line.startswith('#index'):\n",
    "            current_paper['Index ID'] = line[6:].strip()\n",
    "        elif line.startswith('#%'):\n",
    "            if 'References' not in current_paper:\n",
    "                current_paper['References'] = []\n",
    "            current_paper['References'].append(line[2:].strip())\n",
    "        elif line.startswith('#!'):\n",
    "            current_paper['Abstract'] = line[2:].strip()\n",
    "        elif line.isdigit() and current_paper:  # Handle end of current paper\n",
    "            df.append(current_paper)\n",
    "            current_paper = {}\n",
    "    # Add the last paper\n",
    "    if current_paper:\n",
    "        df.append(current_paper)\n",
    "\n",
    "# Convert list of dicts to dfFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Replacing missing keys with None which becomes NaN in dfFrame\n",
    "for key in ['Title', 'Authors', 'Year', 'Venue', 'Index ID', 'References', 'Abstract']:\n",
    "    if key not in df.columns:\n",
    "        df[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "df = df[(df['Year']>1990) & (df['Year']<2010)]\n",
    "df['Index ID'] = pd.to_numeric(df['Index ID'], errors='coerce')\n",
    "# Apply the function to the dfFrame column\n",
    "df['References'] = df['References'].apply(\n",
    "    lambda x: [int(i) for i in x] if isinstance(x, list) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Cited Year (for SIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary from the dfFrame assuming 'Index ID' and 'Year' are columns in your dfFrame\n",
    "year_dict = df.set_index('Index ID')['Year'].to_dict()\n",
    "\n",
    "def get_last_cited_year(ref_ids):\n",
    "    if not isinstance(ref_ids, list):\n",
    "        return np.nan\n",
    "    # Filter and collect years where reference IDs exist in year_dict\n",
    "    years = [year_dict.get(int(ref_id)) for ref_id in ref_ids if int(ref_id) in year_dict]\n",
    "    return max(years) if years else np.nan\n",
    "\n",
    "# Apply the function to the 'References' column to compute the 'Last Cited Year'\n",
    "df['Last Cited Year'] = df['References'].apply(get_last_cited_year)\n",
    "df[\"Venue\"] = df[\"Venue\"].astype(str)\n",
    "df['Venue'] = df['Venue'].replace('', np.nan)\n",
    "df[\"References Count\"] = df[\"References\"].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total authors\n",
    "df['Authors'] = df['Authors'].fillna('')  # Fill empty strings where there are no authors\n",
    "unique_authors = set()\n",
    "df['Authors'].apply(lambda x: unique_authors.update(x.split(',')) if x else None)\n",
    "total_authors = len(unique_authors) - 1 if '' in unique_authors else len(unique_authors)\n",
    "\n",
    "# Fill missing Venue values with \"Unknown\"\n",
    "df['Venue'] = df['Venue'].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Index ID'] = pd.to_numeric(df['Index ID'], errors='coerce')\n",
    "\n",
    "# Convert 'References' from string representation of list to actual list\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    except:\n",
    "        return []  # Return an empty list if there's any error\n",
    "\n",
    "df['References'] = df['References'].apply(safe_eval)\n",
    "\n",
    "# Creating a dictionary of publication years for quick lookup\n",
    "publication_years = df.set_index('Index ID')['Year'].to_dict()\n",
    "\n",
    "# Function to count citations within three years\n",
    "def count_citations_within_3_years(row):\n",
    "    if isinstance(row['References'], list):  # Ensure that the data is list\n",
    "        citation_years = [publication_years.get(int(ref)) for ref in row['References'] if publication_years.get(int(ref)) is not None]\n",
    "        return sum((year is not None and (row['Year'] <= year <= row['Year'] + 3)) for year in citation_years)\n",
    "    return 0  # Return 0 if References is not a list\n",
    "\n",
    "# Apply the function\n",
    "df['citation_count_within_3_years'] = df.apply(count_citations_within_3_years, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define high-impact venues\n",
    "high_impact_venues = [\n",
    "    \"Nature\", \"Science\", \"IEEE Transactions on Information Forensics and Security\",\n",
    "    \"Journal of Machine Learning Research\", \"Communications of the ACM\",\n",
    "    \"IEEE Transactions on Knowledge and Data Engineering\", \"Journal of the ACM\",\n",
    "    \"Proceedings of the IEEE\", \"ACM Transactions on Algorithms\",\n",
    "    \"IEEE Trans. Parallel Distrib. Syst.\", \"IEEE Trans. Evolutionary Computation\",\n",
    "    \"Journal of Computational Physics\", \"Journal of the American Society for Information Science and Technology\"\n",
    "]\n",
    "\n",
    "df[\"High_Impact\"] = df['Venue'].isin(high_impact_venues)\n",
    "\n",
    "# Determine the range of years to simulate\n",
    "min_year = df['Year'].min()\n",
    "max_year = df['Year'].max()\n",
    "\n",
    "# Expand the dataset to have a row for each year for each paper\n",
    "expanded_df = pd.DataFrame([\n",
    "    {**row, 'Simulation_Year': year}\n",
    "    for _, row in df.iterrows()\n",
    "    for year in range(row['Year'], max_year + 1)\n",
    "])\n",
    "\n",
    "# Initialize states for each year\n",
    "def determine_initial_state(row):\n",
    "    if row['High_Impact'] and row['citation_count_within_3_years'] >= 5:\n",
    "        return 'Exposed'\n",
    "    return 'Published'\n",
    "\n",
    "expanded_df['State'] = expanded_df.apply(determine_initial_state, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Iterate through each year and update the states\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_year, max_year \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Apply state updates for each paper for the specific year\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     expanded_df\u001b[38;5;241m.\u001b[39mloc[expanded_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimulation_Year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSimulation_Year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndex ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndex ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSimulation_Year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mState\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_year\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mState\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:10347\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10333\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10335\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10337\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10345\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10346\u001b[0m )\n\u001b[1;32m> 10347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Iterate through each year and update the states\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_year, max_year \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Apply state updates for each paper for the specific year\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     expanded_df\u001b[38;5;241m.\u001b[39mloc[expanded_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimulation_Year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m expanded_df[expanded_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimulation_Year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m---> 32\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m row: update_state(row, \u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndex ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndex ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSimulation_Year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mState\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m year \u001b[38;5;241m>\u001b[39m min_year \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     33\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1192\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1191\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1753\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1753\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\AndrewsMatthewHaines\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1686\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1684\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Define high-impact venues and thresholds\n",
    "high_impact_venues = [\n",
    "    \"Nature\", \"Science\", \"IEEE Transactions on Information Forensics and Security\",\n",
    "    \"Journal of Machine Learning Research\", \"Communications of the ACM\",\n",
    "    \"IEEE Transactions on Knowledge and Data Engineering\", \"Journal of the ACM\",\n",
    "    \"Proceedings of the IEEE\", \"ACM Transactions on Algorithms\",\n",
    "    \"IEEE Trans. Parallel Distrib. Syst.\", \"IEEE Trans. Evolutionary Computation\",\n",
    "    \"Journal of Computational Physics\", \"Journal of the American Society for Information Science and Technology\"\n",
    "]\n",
    "\n",
    "# Function to check if the venue is high-impact\n",
    "def is_high_impact(venue):\n",
    "    return any(high in venue for high in high_impact_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(row, previous_state):\n",
    "    if previous_state == 'Published':\n",
    "        if row['citation_count_within_3_years'] >= 5 or is_high_impact(row['Venue']):\n",
    "            return 'Exposed'\n",
    "        elif isinstance(row['References'], list):\n",
    "            return 'Cited'\n",
    "    elif previous_state == 'Exposed' and row['Simulation_Year'] - row['Year'] > 3:\n",
    "        if row['citation_count_within_3_years'] < 5:\n",
    "            return 'Archived'\n",
    "    return previous_state\n",
    "\n",
    "# Iterate through each year and update the states\n",
    "for year in range(min_year, max_year + 1):\n",
    "    # Apply state updates for each paper for the specific year\n",
    "    expanded_df.loc[expanded_df['Simulation_Year'] == year, 'State'] = expanded_df[expanded_df['Simulation_Year'] == year].apply(\n",
    "        lambda row: update_state(\n",
    "            row,\n",
    "            expanded_df[\n",
    "                (expanded_df['Index ID'] == row['Index ID']) &\n",
    "                (expanded_df['Simulation_Year'] == year - 1)\n",
    "            ]['State'].iloc[0] if not expanded_df[\n",
    "                (expanded_df['Index ID'] == row['Index ID']) &\n",
    "                (expanded_df['Simulation_Year'] == year - 1)\n",
    "            ].empty else row['State']\n",
    "        ),\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.to_csv('expanded_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) PACEU model <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = 2000\n",
    "test_year = 2007\n",
    "max_year = 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split\n",
    "train_df = df[(df['Year'] <= test_year) & (df['Year'] > min_year)]\n",
    "test_df = df[df['Year'] > test_year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Define model and calculate transition rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citation_model(y, t, lambda_PE, lambda_EC, lambda_CH, gamma_HA):\n",
    "    P, E, C, H, A = y\n",
    "    dP_dt = -lambda_PE * P  # Rate at which papers move from PENDING to EXPOSED\n",
    "    dE_dt = lambda_PE * P - lambda_EC * E  # New papers becoming EXPOSED and some moving to CITED\n",
    "    dC_dt = lambda_EC * E - lambda_CH * C  # EXPOSED papers moving to CITED or to HIGH IMPACT\n",
    "    dH_dt = lambda_CH * C - gamma_HA * H  # CITED papers becoming HIGH IMPACT and some moving to ARCHIVED\n",
    "    dA_dt = gamma_HA * H  # HIGH IMPACT papers becoming ARCHIVED\n",
    "    return [dP_dt, dE_dt, dC_dt, dH_dt, dA_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_rates(data, from_state, to_state):\n",
    "    # Adjust data to match state names and logic\n",
    "    data['Next_Year'] = data['Year'] + 1\n",
    "    from_next_year = data[['Index ID', 'Next_Year', to_state]].rename(columns={'Next_Year': 'Year', to_state: 'To_State_Next_Year'})\n",
    "    merged = data.merge(from_next_year, on=['Index ID', 'Year'], how='left')\n",
    "    \n",
    "    # Calculate transitions based on the new state definitions\n",
    "    transitions = merged[(merged['State'] == from_state) & (merged['To_State_Next_Year'] == to_state)]\n",
    "    total_from_state = merged[merged['State'] == from_state]\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    rate = len(transitions) / len(total_from_state) if len(total_from_state) > 0 else 0\n",
    "    return rate\n",
    "\n",
    "# Calculate parameter estimates for each transition\n",
    "lambda_PE = calculate_transition_rates(train_df, 'Published', 'Exposed')\n",
    "lambda_EC = calculate_transition_rates(train_df, 'Exposed', 'Cited')\n",
    "lambda_CH = calculate_transition_rates(train_df, 'Cited', 'High Impact')\n",
    "gamma_HA = calculate_transition_rates(train_df, 'High Impact', 'Archived')\n",
    "\n",
    "# Print the estimated parameters\n",
    "print(f\"Estimated lambda_PE: {lambda_PE}\")\n",
    "print(f\"Estimated lambda_EC: {lambda_EC}\")\n",
    "print(f\"Estimated lambda_CH: {lambda_CH}\")\n",
    "print(f\"Estimated gamma_HA: {gamma_HA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Set initial conditions and run on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions based on the first year data in training set\n",
    "initial_conditions_train = [\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('Published', 0),\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('Exposed', 0),\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('Cited', 0),\n",
    "    train_df[train_df['Year'] == min_year]['State'].value_counts().get('High Impact', 0)\n",
    "]\n",
    "\n",
    "# Time points\n",
    "time_points_train = np.arange(min_year, test_year + 1)\n",
    "\n",
    "# Solve ODE for training data\n",
    "results_train = odeint(citation_model, initial_conditions_train, time_points_train, args=(lambda_PE, lambda_EC, lambda_CH, gamma_HA))\n",
    "\n",
    "# Gather actual data for training period\n",
    "actual_data_train = [\n",
    "    [train_df[train_df['Year'] == year]['State'].value_counts().get('Published', 0),\n",
    "     train_df[train_df['Year'] == year]['State'].value_counts().get('Exposed', 0),\n",
    "     train_df[train_df['Year'] == year]['State'].value_counts().get('Cited', 0),\n",
    "     train_df[train_df['Year'] == year]['State'].value_counts().get('High Impact', 0)] for year in time_points_train\n",
    "]\n",
    "\n",
    "# Calculate MSE for training\n",
    "mse_train = np.mean((np.array(actual_data_train) - results_train)**2)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(time_points_train, results_train, marker='o', label=['Simulated Published', 'Simulated Exposed', 'Simulated Cited', 'Simulated High Impact'])\n",
    "plt.plot(time_points_train, np.array(actual_data_train), linestyle='--', label=['Actual Published', 'Actual Exposed', 'Actual Cited', 'Actual High Impact'])\n",
    "plt.title('Training Set: Actual vs. Simulated Data')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Squared Error for Training Data: {mse_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions based on last year of training data\n",
    "initial_conditions_test = results_train[-1, :]\n",
    "\n",
    "# Time points for testing data\n",
    "time_points_test = np.arange(test_year + 1, max_year + 1)\n",
    "\n",
    "# Solve ODE for testing data\n",
    "results_test = odeint(citation_model, initial_conditions_test, time_points_test, args=(lambda_PE, lambda_EC, lambda_CH, gamma_HA))\n",
    "\n",
    "# Gather actual data for testing period\n",
    "actual_data_test = [\n",
    "    [test_df[test_df['Year'] == year]['State'].value_counts().get('Published', 0),\n",
    "     test_df[test_df['Year'] == year]['State'].value_counts().get('Exposed', 0),\n",
    "     test_df[test_df['Year'] == year]['State'].value_counts().get('Cited', 0),\n",
    "     test_df[test_df['Year'] == year]['State'].value_counts().get('High Impact', 0)] for year in time_points_test\n",
    "]\n",
    "\n",
    "# Calculate MSE for testing\n",
    "mse_test = np.mean((np.array(actual_data_test) - results_test)**2)\n",
    "\n",
    "# Plot testing results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(time_points_test, results_test, marker='o', label=['Simulated Published', 'Simulated Exposed', 'Simulated Cited', 'Simulated High Impact'])\n",
    "plt.plot(time_points_test, np.array(actual_data_test), linestyle='--', label=['Actual Published', 'Actual Exposed', 'Actual Cited', 'Actual High Impact'])\n",
    "plt.title('Testing Set: Actual vs. Simulated Data')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Squared Error for Testing Data: {mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sensitivity <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range for each parameter\n",
    "lambda_PE_values = np.linspace(0.05, 0.15, 5)\n",
    "lambda_EC_values = np.linspace(0.02, 0.08, 5)\n",
    "lambda_CH_values = np.linspace(0.01, 0.05, 5)\n",
    "gamma_HA_values = np.linspace(0.005, 0.015, 5)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for lambda_PE in lambda_PE_values:\n",
    "    for lambda_EC in lambda_EC_values:\n",
    "        for lambda_CH in lambda_CH_values:\n",
    "            for gamma_HA in gamma_HA_values:\n",
    "                # Solve ODE with current parameters\n",
    "                results_sim = odeint(citation_model, initial_conditions_train, time_points_train, args=(lambda_PE, lambda_EC, lambda_CH, gamma_HA))\n",
    "                mse_sim = np.mean((np.array(actual_data_train) - results_sim)**2)\n",
    "                results.append((lambda_PE, lambda_EC, lambda_CH, gamma_HA, mse_sim))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['lambda_PE', 'lambda_EC', 'lambda_CH', 'gamma_HA', 'MSE'])\n",
    "\n",
    "# Find the parameters that minimize MSE\n",
    "min_mse_params = results_df.loc[results_df['MSE'].idxmin()]\n",
    "\n",
    "print(\"Optimal parameters:\")\n",
    "print(min_mse_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
